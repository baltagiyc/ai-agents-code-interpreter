# =============================================================================
# EXEMPLE CONCRET : ASSISTANT DE RECHERCHE AVEC STATE CUSTOM
# =============================================================================
#
# CAS D'USAGE RÃ‰EL : Tu poses une question, l'agent :
# 1. Cherche sur le web (Tavily)
# 2. Analyse les sources trouvÃ©es
# 3. GÃ©nÃ¨re un rapport structurÃ©
#
# Le State custom permet de tracker :
# - Combien de recherches ont Ã©tÃ© faites
# - Quelles sources ont Ã©tÃ© consultÃ©es
# - L'Ã©volution de la rÃ©ponse
# =============================================================================

import sys
from pathlib import Path
from typing import TypedDict, Annotated
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from dotenv import load_dotenv

load_dotenv()

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_tavily import TavilySearch

# =============================================================================
# 1. STATE CUSTOM - On dÃ©finit TOUT ce qu'on veut tracker
# =============================================================================


class ResearchState(TypedDict):
    """
    Notre State personnalisÃ© pour l'agent de recherche.

    Pourquoi pas juste MessagesState ?
    â†’ Parce qu'on veut VOIR ce qui se passe :
      combien de recherches, quelles sources, quelle confiance...
    """

    # La conversation (messages LLM) - avec add_messages pour accumuler
    messages: Annotated[list[BaseMessage], add_messages]

    # La question originale de l'utilisateur
    user_question: str

    # Les sources trouvÃ©es par Tavily
    sources_found: list[dict]  # [{url, title, content}, ...]

    # Compteur de recherches effectuÃ©es
    search_count: int

    # Le rÃ©sumÃ© final gÃ©nÃ©rÃ©
    final_summary: str

    # Score de confiance (1-10)
    confidence_score: int

    # Ã‰tape actuelle pour debug
    current_step: str


# =============================================================================
# 2. CONFIGURATION
# =============================================================================

# Le LLM pour analyser et rÃ©diger
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# Le tool de recherche web
tavily = TavilySearch(max_results=3)


# =============================================================================
# 3. LES NODES - Chaque Ã©tape du workflow
# =============================================================================


def recherche_web(state: ResearchState) -> dict:
    """
    NODE 1 : Fait une recherche web avec Tavily.

    EntrÃ©e : La question de l'utilisateur
    Sortie : Liste de sources avec leurs contenus
    """
    print(f"\nðŸ” RECHERCHE WEB pour : '{state['user_question']}'")

    # Appel Ã  Tavily
    results = tavily.invoke(state["user_question"])

    # Tavily retourne une string, on la parse
    # En vrai, TavilySearch retourne directement les rÃ©sultats structurÃ©s
    # mais pour l'exemple on simule la structure

    sources = []
    if isinstance(results, str):
        # Si c'est une string, on crÃ©e une source fictive
        sources = [{"url": "tavily_search", "title": "RÃ©sultats", "content": results}]
    elif isinstance(results, list):
        sources = results
    else:
        sources = [{"url": "unknown", "title": "RÃ©sultat", "content": str(results)}]

    print(f"   âœ… {len(sources)} source(s) trouvÃ©e(s)")
    for i, src in enumerate(sources[:3]):
        if isinstance(src, dict):
            print(f"   {i+1}. {src.get('title', src.get('url', 'Source'))[:50]}...")

    # On retourne les modifications du State
    return {
        "sources_found": sources,
        "search_count": state["search_count"] + 1,
        "current_step": "recherche_terminÃ©e",
        "messages": [
            AIMessage(content=f"J'ai trouvÃ© {len(sources)} source(s) pertinente(s).")
        ],
    }


def analyse_sources(state: ResearchState) -> dict:
    """
    NODE 2 : Analyse les sources avec le LLM.

    EntrÃ©e : Les sources trouvÃ©es
    Sortie : Un score de confiance et une analyse
    """
    print(f"\nðŸ§  ANALYSE des {len(state['sources_found'])} sources...")

    # PrÃ©pare le contexte pour le LLM
    sources_text = "\n\n".join(
        [
            f"Source {i+1}:\n{src.get('content', str(src))[:500]}"
            for i, src in enumerate(state["sources_found"][:3])
        ]
    )

    analysis_prompt = f"""Analyse ces sources pour rÃ©pondre Ã  la question : "{state['user_question']}"

SOURCES :
{sources_text}

RÃ©ponds en JSON avec ce format :
{{"confidence": 1-10, "key_facts": ["fait 1", "fait 2"], "analysis": "ton analyse"}}
"""

    response = llm.invoke(
        [
            SystemMessage(
                content="Tu es un analyste expert. RÃ©ponds uniquement en JSON valide."
            ),
            HumanMessage(content=analysis_prompt),
        ]
    )

    # Parse basique (en prod tu utiliserais un parser JSON)
    content = response.content

    # Extraire la confiance (simpliste)
    confidence = 7  # Par dÃ©faut
    if '"confidence":' in content:
        try:
            import re

            match = re.search(r'"confidence":\s*(\d+)', content)
            if match:
                confidence = int(match.group(1))
        except:
            pass

    print(f"   âœ… Analyse terminÃ©e - Confiance : {confidence}/10")

    return {
        "confidence_score": confidence,
        "current_step": "analyse_terminÃ©e",
        "messages": [
            AIMessage(
                content=f"Analyse terminÃ©e. Confiance : {confidence}/10\n{content}"
            )
        ],
    }


def genere_rapport(state: ResearchState) -> dict:
    """
    NODE 3 : GÃ©nÃ¨re le rapport final.

    EntrÃ©e : L'analyse et les sources
    Sortie : Un rÃ©sumÃ© structurÃ©
    """
    print(f"\nðŸ“ GÃ‰NÃ‰RATION du rapport final...")

    rapport_prompt = f"""Question originale : {state['user_question']}

BasÃ© sur {len(state['sources_found'])} sources analysÃ©es avec une confiance de {state['confidence_score']}/10.

GÃ©nÃ¨re un rapport structurÃ© avec :
1. **RÃ©ponse courte** (2-3 phrases)
2. **Points clÃ©s** (liste Ã  puces)
3. **Limites** (ce qu'on ne sait pas)

Sois concis et factuel.
"""

    response = llm.invoke(
        [
            SystemMessage(
                content="Tu es un rÃ©dacteur expert. Structure tes rÃ©ponses clairement."
            ),
            HumanMessage(content=rapport_prompt),
        ]
    )

    print(f"   âœ… Rapport gÃ©nÃ©rÃ© ({len(response.content)} caractÃ¨res)")

    return {
        "final_summary": response.content,
        "current_step": "rapport_gÃ©nÃ©rÃ©",
        "messages": [response],
    }


# =============================================================================
# 4. CONDITIONS - Logique de dÃ©cision
# =============================================================================


def faut_il_reanalyser(state: ResearchState) -> str:
    """
    DÃ©cide si on doit refaire une analyse ou passer au rapport.

    Logique :
    - Si confiance < 5 ET moins de 2 recherches â†’ refaire une recherche
    - Sinon â†’ gÃ©nÃ©rer le rapport
    """
    print(
        f"\nðŸ¤” DÃ‰CISION : Confiance={state['confidence_score']}, Recherches={state['search_count']}"
    )

    if state["confidence_score"] < 5 and state["search_count"] < 2:
        print("   â†’ Confiance trop basse, nouvelle recherche")
        return "recherche"

    print("   â†’ Confiance OK, gÃ©nÃ©ration du rapport")
    return "rapport"


# =============================================================================
# 5. CONSTRUCTION DU GRAPH
# =============================================================================

# CrÃ©ation avec notre State custom
graph = StateGraph(ResearchState)

# Ajout des nodes (chaque Ã©tape)
graph.add_node("recherche", recherche_web)
graph.add_node("analyse", analyse_sources)
graph.add_node("rapport", genere_rapport)

# Point d'entrÃ©e : on commence par la recherche
graph.set_entry_point("recherche")

# AprÃ¨s recherche â†’ toujours analyse
graph.add_edge("recherche", "analyse")

# AprÃ¨s analyse â†’ dÃ©cision (refaire recherche ou gÃ©nÃ©rer rapport)
graph.add_conditional_edges(
    "analyse",
    faut_il_reanalyser,
    {
        "recherche": "recherche",  # Boucle si confiance basse
        "rapport": "rapport",  # Sinon rapport final
    },
)

# AprÃ¨s rapport â†’ FIN
graph.add_edge("rapport", END)

# Compilation
app = graph.compile()

# Sauvegarde du graph en image
try:
    app.get_graph().draw_mermaid_png(output_file_path="research_agent_flow.png")
    print("ðŸ“Š Graph sauvegardÃ© dans research_agent_flow.png")
except Exception as e:
    print(f"âš ï¸ Impossible de gÃ©nÃ©rer l'image : {e}")


# =============================================================================
# 6. EXÃ‰CUTION
# =============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ðŸš€ AGENT DE RECHERCHE - Exemple avec State Custom")
    print("=" * 60)

    # La question de l'utilisateur
    question = "Quels sont les derniers dÃ©veloppements de LangGraph en dÃ©cembre 2024 ?"

    # STATE INITIAL - C'est TOI qui initialises tous les champs
    initial_state = {
        "messages": [HumanMessage(content=question)],
        "user_question": question,
        "sources_found": [],  # Vide au dÃ©part
        "search_count": 0,  # Pas encore de recherche
        "final_summary": "",  # Pas encore de rÃ©sumÃ©
        "confidence_score": 0,  # Pas encore de score
        "current_step": "dÃ©marrage",  # Ã‰tape initiale
    }

    print(f"\nðŸ“‹ Question : {question}")
    print("-" * 60)

    # ExÃ©cution de l'agent
    result = app.invoke(initial_state)

    # ==========================================================================
    # AFFICHAGE DU RÃ‰SULTAT - GrÃ¢ce au State custom, on a TOUT
    # ==========================================================================

    print("\n" + "=" * 60)
    print("ðŸ“Š RÃ‰SULTAT FINAL")
    print("=" * 60)

    print(f"\nðŸ”¢ Statistiques :")
    print(f"   â€¢ Recherches effectuÃ©es : {result['search_count']}")
    print(f"   â€¢ Sources trouvÃ©es : {len(result['sources_found'])}")
    print(f"   â€¢ Score de confiance : {result['confidence_score']}/10")
    print(f"   â€¢ Ã‰tape finale : {result['current_step']}")
    print(f"   â€¢ Messages gÃ©nÃ©rÃ©s : {len(result['messages'])}")

    print(f"\nðŸ“ Rapport final :")
    print("-" * 60)
    print(result["final_summary"])

    print("\n" + "=" * 60)
    print("ðŸ’¡ AVEC MessagesState, tu n'aurais eu QUE les messages,")
    print("   pas les stats, pas les sources, pas le score de confiance !")
    print("=" * 60)
